{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "id_label = 'id'\n",
    "text_label = 'comment_text'\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train_file = 'train_pp.csv'\n",
    "val_file = 'val_pp.csv'\n",
    "test_file = 'test_pp.csv'\n",
    "\n",
    "extern_cols = ['toxicity', 'aggression', 'attack']\n",
    "extern_mask_cols = ['mask_tox', 'mask_agg', 'mask_att']\n",
    "extern_text_label = 'comment'\n",
    "train_ext_file = 'train_external.csv'\n",
    "val_ext_file = 'val_external.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "# some iterators produce StopIteration, which is no longer a warning, we don't need to hear about it\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "from torchtext.vocab import Vectors\n",
    "import re\n",
    "import io,os,csv\n",
    "\n",
    "class ToxicDataset(data.Dataset):\n",
    "    \"\"\"Defines a Dataset of columns stored in CSV format.\"\"\"\n",
    "\n",
    "    def __init__(self, path, fields, skip_header=True, **kwargs):\n",
    "        with io.open(os.path.expanduser(path), encoding=\"utf8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "                \n",
    "            if skip_header:\n",
    "                next(reader)\n",
    "\n",
    "            examples = [data.Example.fromlist(line, fields) for line in reader]\n",
    "\n",
    "        super(ToxicDataset, self).__init__(examples, fields, **kwargs)\n",
    "        \n",
    "re_symbol = re.compile(r'[^\\w ]')\n",
    "def tokenize(x):\n",
    "    x = re_symbol.sub('',str(x))\n",
    "    a = x.split()\n",
    "    if len(a) <= 0:\n",
    "        a = ['<empty>']\n",
    "    return a\n",
    "\n",
    "class ToxData:\n",
    "    def __init__(self, path):\n",
    "\n",
    "        # Define all the types of fields\n",
    "        # pip install spacy for the tokenizer to work (or remove to use default)\n",
    "        self.TEXT = data.Field(lower=True, include_lengths=True, tokenize=tokenize)\n",
    "        # import dill as pickle\n",
    "        # TEXT = pickle.load(open(f'{path}TEXT_wlm.pkl','rb'))\n",
    "        self.LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "        # we use the index field to re-sort test data after processing\n",
    "        self.INDEX = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "        train_fields=[\n",
    "            (id_label, self.INDEX),\n",
    "            ('nid', None),\n",
    "            (text_label, self.TEXT)\n",
    "        ]\n",
    "        for label in label_cols:\n",
    "            train_fields.append((label,self.LABEL))\n",
    "\n",
    "        self.train_data, self.val_data = ToxicDataset.splits(\n",
    "                    path=path, train=train_file, validation=val_file,\n",
    "                    fields=train_fields\n",
    "                )\n",
    "\n",
    "        test_fields=[\n",
    "            (id_label, self.INDEX),\n",
    "            ('nid', None),\n",
    "            (text_label, self.TEXT)\n",
    "        ]\n",
    "        self.test_data = ToxicDataset(\n",
    "                    path=f'{path}{test_file}',\n",
    "                    fields=test_fields\n",
    "                )\n",
    "        self.LABEL_FLOAT = data.Field(sequential=False, use_vocab=False, tensor_type=torch.cuda.FloatTensor)\n",
    "        extern_fields=[\n",
    "            (id_label, None),\n",
    "            (extern_text_label, self.TEXT)\n",
    "        ]\n",
    "        for label in extern_cols:\n",
    "            extern_fields.append((label,self.LABEL_FLOAT))\n",
    "        for label in extern_mask_cols:\n",
    "            extern_fields.append((label,self.LABEL))\n",
    "        self.train_ext_data, self.val_ext_data = ToxicDataset.splits(\n",
    "                    path=path, train=train_ext_file, validation=val_ext_file,\n",
    "                    fields=extern_fields\n",
    "                )\n",
    "        \n",
    "    def build_vocab(self,vocab):\n",
    "        # This will download the glove vectors, see torchtext source for other options\n",
    "        max_size = 500000\n",
    "        self.TEXT.build_vocab(self.train_data, self.val_data, self.test_data, vectors=Vectors(vocab), max_size=max_size)\n",
    "        self.INDEX.build_vocab()\n",
    "\n",
    "        # print vocab information\n",
    "        self.ntokens = len(self.TEXT.vocab)\n",
    "        return self.ntokens\n",
    "    \n",
    "    def make_iter(self):\n",
    "        self.train = data.BucketIterator(self.train_data, batch_size=32,\n",
    "                                sort_key=lambda x: len(x.comment_text),\n",
    "                                sort_within_batch=True, repeat=False)\n",
    "        self.val = data.BucketIterator(self.val_data, batch_size=32,\n",
    "                                        sort_key=lambda x: len(x.comment_text),\n",
    "                                        sort_within_batch=True, train=False, repeat=False)\n",
    "        self.test = data.BucketIterator(self.test_data, batch_size=128,\n",
    "                                        sort_key=lambda x: len(x.comment_text),\n",
    "                                        sort_within_batch=True, train=False, repeat=False)\n",
    "        self.train_ext = data.BucketIterator(self.train_ext_data, batch_size=64,\n",
    "                                sort_key=lambda x: len(x.comment),\n",
    "                                sort_within_batch=True, repeat=True)\n",
    "        self.val_ext = data.BucketIterator(self.val_ext_data, batch_size=128,\n",
    "                                        sort_key=lambda x: len(x.comment),\n",
    "                                        sort_within_batch=True, train=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BucketIterator will shuffle the data and produce batches with sequences of roughly the same length. If we didn't want to split into epochs, we could set repeat=True and run for a set number of batches (rather than epochs). Must have `sort_within_batch=True` to use the lengths we picked up earlier.\n",
    "\n",
    "We also define convenience methods to access the comment text and labels from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_text(batch):\n",
    "    return getattr(batch, text_label)\n",
    "def get_labels(batch):\n",
    "    # Get the labels as one tensor from the batch object\n",
    "    return torch.cat([getattr(batch, label).unsqueeze(1) for label in label_cols], dim=1).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_ext_text(batch):\n",
    "    return getattr(batch, extern_text_label)\n",
    "def get_ext_labels(batch):\n",
    "    # Get the labels as one tensor from the batch object\n",
    "    return torch.cat([getattr(batch, label).unsqueeze(1) for label in extern_cols], dim=1).float()\n",
    "def get_ext_mask(batch):\n",
    "    # Get the labels as one tensor from the batch object\n",
    "    return torch.cat([getattr(batch, label).unsqueeze(1) for label in extern_mask_cols], dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl=ToxData('./toxic-data/')\n",
    "dl.build_vocab('crawl-300d-2M.vec')\n",
    "dl.make_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 96.2396\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch=next(iter(dl.train_ext))\n",
    "get_ext_text(batch)\n",
    "y=get_ext_labels(batch)\n",
    "w=get_ext_mask(batch)\n",
    "l=torch.nn.L1Loss(reduce=False)(y,torch.ones_like(w))\n",
    "torch.sum(l.mul(w))\n",
    "\n",
    "#train_ext_data[0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the meat of the model. A few points to notice in `__init__`:\n",
    "- `Dropout2d` is a spatial dropout function, which will drop entire layers (rather than just individial connections). It doesn't necessarily require 2d data\n",
    "- We define `self.rnns` as a ModuleList so that all of the sub-components will be discovered properly\n",
    "- The pools require an argument that is number of output segments, but we just want a global one for each avg/max\n",
    "\n",
    "and in `forward`:\n",
    "- We move to/from a packed sequence for the rnn section if we have the lengths\n",
    "- We need to rearrange the output of the rnn to have sequence last for pooling layers\n",
    "- We don't have a sigmoid output because we will later use a special loss function that takes the logit output directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, ndense, nout, nlayers, dropemb=0.2, droprnn=0.0, droplin=0.0, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.dropemb = nn.Dropout2d(dropemb)\n",
    "        self.ndir = 2 if bidirectional else 1\n",
    "        assert rnn_type in ['LSTM', 'GRU'], 'RNN type is not supported'\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid*self.ndir, nhid, 1, dropout=droprnn, bidirectional=bidirectional) for l in range(nlayers)]\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnns = [torch.nn.GRU(ninp if l == 0 else nhid*self.ndir, nhid, 1, dropout=droprnn, bidirectional=bidirectional) for l in range(nlayers)]\n",
    "        \n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.avg_pool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = torch.nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.droplin = nn.Dropout(droplin)\n",
    "        self.dense = nn.Linear(nhid*self.ndir, ndense) #*2 if pooling\n",
    "        self.decoder_three = nn.Linear(ndense, 3) \n",
    "        self.decoder_six = nn.Linear(ndense, 6) \n",
    "        \n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "        emb = self.encoder(input)\n",
    "        \n",
    "        raw_output = self.dropemb(emb)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            lengths = lengths.view(-1).tolist()\n",
    "            raw_output = nn.utils.rnn.pack_padded_sequence(raw_output, lengths)\n",
    "            \n",
    "        for rnn in self.rnns:\n",
    "            raw_output,_ = rnn(raw_output)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            raw_output, lengths = nn.utils.rnn.pad_packed_sequence(raw_output)\n",
    "            \n",
    "        bsz = raw_output.size(1)\n",
    "#         rnn_avg = self.avg_pool(raw_output.permute(1,2,0))\n",
    "#         rnn_max = self.max_pool(raw_output.permute(1,2,0))\n",
    "#         rnn_out = torch.cat([rnn_avg.view(bsz,-1),rnn_max.view(bsz,-1)], dim=1)\n",
    "        \n",
    "        rnn_out = raw_output[0].view(bsz,-1)\n",
    "        if (self.ndir == 2):\n",
    "            rnn_fwd = raw_output[-1,:,:self.nhid]\n",
    "            rnn_rev = raw_output[0,:,self.nhid:]\n",
    "            rnn_out = torch.cat([rnn_fwd,rnn_rev], dim=1)\n",
    "            \n",
    "        dense_out=self.dense(self.droplin(rnn_out))\n",
    "        \n",
    "        return dense_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchnet\n",
    "class MultiAUCMeter(object):\n",
    "    def __init__(self, n):\n",
    "        self.meters = []\n",
    "        self.n = n\n",
    "        for i in range(n):\n",
    "            self.meters.append(torchnet.meter.AUCMeter())\n",
    "    def reset(self):\n",
    "        for meter in self.meters:\n",
    "            meter.reset()\n",
    "    def add(self, preds, targets):\n",
    "        targets = targets.data.cpu().numpy()\n",
    "        preds = preds.data.cpu().numpy()\n",
    "        for i in range(self.n):\n",
    "            self.meters[i].add(preds[:,i], targets[:,i])\n",
    "    def avg(self):\n",
    "        total = 0.0\n",
    "        for meter in self.meters:\n",
    "            value, _, _ = meter.value()\n",
    "            total += value\n",
    "        return total/self.n\n",
    "    def values(self):\n",
    "        return [m.value()[0] for m in self.meters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are parameters from other example kernels--not necessarily optimized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "nhidden=160\n",
    "emsize=300\n",
    "nlayers = 2\n",
    "dropemb = 0.0\n",
    "droprnn = 0.2\n",
    "droplin = 0.0\n",
    "model = RNNModel('GRU', ntokens, emsize, nhidden, 16, 3, nlayers, dropemb=dropemb, droprnn=droprnn, droplin=droplin, bidirectional=True)\n",
    "model.encoder.weight.data.copy_(TEXT.vocab.vectors)\n",
    "#model.encoder.load_state_dict(torch.load(f'{path}encoder_wlm.pt'))\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))\n",
    "model.encoder.weight.requires_grad=False\n",
    "optimizer = optim.Adam([p for p in model.parameters()][1:], lr=3e-4, betas=(0.7, 0.99))\n",
    "if use_cuda:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion=nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main pytorch training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_ext = torch.nn.MSELoss(reduce=False)\n",
    "criterion_reg=nn.BCEWithLogitsLoss()\n",
    "def get_opt(model):\n",
    "    optim_reg = optim.Adam([p for p in model.parameters()][1:], lr=3e-4, betas=(0.7, 0.99))\n",
    "    optim_ext = optim.Adam([p for p in model.parameters()][1:], lr=3e-4, betas=(0.7, 0.99))    \n",
    "    return optim_reg,optim_ext\n",
    "    \n",
    "def  train_batch(model, batch, opts):\n",
    "    opt_reg,opt_ext=opts\n",
    "    ext_data=False\n",
    "    if hasattr(batch,'comment'):\n",
    "        ext_data=True\n",
    "    if ext_data:\n",
    "        (x,xl) = get_ext_text(batch)\n",
    "        y = get_ext_labels(batch)\n",
    "        w = get_ext_mask(batch)\n",
    "        \n",
    "        opt_ext.zero_grad()\n",
    "        \n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = F.sigmoid(model.decoder_three(model_out))\n",
    "        loss_mat = criterion_ext(preds, y).mul_(w)\n",
    "        loss = torch.sum(loss_mat)\n",
    "        loss.backward()\n",
    "        opt_ext.step()\n",
    "        return 0\n",
    "    else:\n",
    "        (x,xl) = get_text(batch)\n",
    "        y = get_labels(batch)\n",
    "        \n",
    "        \n",
    "        opt_reg.zero_grad()\n",
    "        \n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = model.decoder_six(model_out)\n",
    "        \n",
    "        loss = criterion_reg(preds, y)\n",
    "        loss.backward()\n",
    "        opt_reg.step()\n",
    "        return loss.data[0]\n",
    "\n",
    "class combine_iters:\n",
    "    def __init__(self, iter1, iter2):\n",
    "        self.iter1 = iter1\n",
    "        self.iter2 = iter2\n",
    "    def __iter__(self):\n",
    "        self.i1 = iter(self.iter1)\n",
    "        self.i2 = iter(self.iter2)\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        return next(np.random.choice([self.i1,self.i2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=combine_iters(train,train_ext)\n",
    "batch=next(iter(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c05f0da68245e193f78176761b7276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "epochs = 4\n",
    "train_both=combine_iters(train,train_ext)\n",
    "opts=get_opt(model)\n",
    "if use_cuda:\n",
    "    criterion=criterion.cuda()\n",
    "val_meter = MultiAUCMeter(6)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    model.train() \n",
    "    t = (tqdm(train_both))\n",
    "    for i,batch in enumerate(t):\n",
    "        loss_data = train_batch(model, batch, opts)\n",
    "        if (loss_data > 0):\n",
    "            running_loss += loss_data\n",
    "            running_count += 1\n",
    "            t.set_postfix(loss=(running_loss/running_count))\n",
    "        \n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_count = 0\n",
    "    val_meter.reset()\n",
    "    for batch in val:\n",
    "        (x,xl) = get_text(batch)\n",
    "        y = get_labels(batch)\n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = model.decoder_six(model_out)\n",
    "        \n",
    "        loss = criterion(preds, y)\n",
    "        \n",
    "        val_loss += loss.data[0]*len(x)\n",
    "        val_count += len(x)\n",
    "        \n",
    "        val_meter.add(preds,y)\n",
    "        \n",
    "    epoch_loss = running_loss / running_count\n",
    "\n",
    "    print('Epoch: {}, Train Loss: {:.5f}, Val Loss: {:.5f}, Val AUC: {:.5f}'.format(epoch, epoch_loss, val_loss/val_count, val_meter.avg()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.98491247519009295,\n",
       " 0.99142120234843145,\n",
       " 0.9929592848428539,\n",
       " 0.99335421016005565,\n",
       " 0.98737421728431163,\n",
       " 0.98692318461323092]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_meter.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a8672443d845f482a1d59a15dcc12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 2.40372, Val Loss: 1.60996, Tox AUC: 0.98158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df69d0c1639541cf8004680417927052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, Train Loss: 1.85265, Val Loss: 1.54917, Tox AUC: 0.98232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64b1cc4b0f34ff19d19411b97b884b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, Train Loss: 1.73819, Val Loss: 1.55202, Tox AUC: 0.98277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4284d01056514f7d82ecc6bf5bcfe10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, Train Loss: 1.64156, Val Loss: 1.47239, Tox AUC: 0.98339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673b765f83324aae8e7a9cd3bf86bacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5, Train Loss: 1.57524, Val Loss: 1.48783, Tox AUC: 0.98412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc303114577f4ea3a27edd746ae79e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6, Train Loss: 1.50035, Val Loss: 1.49340, Tox AUC: 0.98464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52dd26e246a4fc290b810a20e37f7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7, Train Loss: 1.43389, Val Loss: 1.62279, Tox AUC: 0.98446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43fe34f1d2b4bd58daf0597b385bf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-61fae363efaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ext_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ext_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 yield Batch(minibatch, self.dataset, self.device,\n\u001b[1;32m    180\u001b[0m                             self.train)\n",
      "\u001b[0;32m<ipython-input-9-ff0a1d243bb7>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m train_ext = data.BucketIterator(train_ext_data, batch_size=64,\n\u001b[0;32m----> 2\u001b[0;31m                                 \u001b[0msort_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                                 sort_within_batch=True, repeat=False)\n\u001b[1;32m      4\u001b[0m val_ext = data.BucketIterator(val_ext_data, batch_size=64,\n\u001b[1;32m      5\u001b[0m                                 \u001b[0msort_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "epochs = 10\n",
    "criterion = torch.nn.MSELoss(reduce=False)\n",
    "if use_cuda:\n",
    "    criterion=criterion.cuda()\n",
    "val_meter = MultiAUCMeter(6)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    model.train() \n",
    "    t = tqdm(train_ext)\n",
    "    for batch in t:\n",
    "        (x,xl) = get_ext_text(batch)\n",
    "        y = get_ext_labels(batch)\n",
    "        w = get_ext_mask(batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = F.sigmoid(model.decoder_three(model_out))\n",
    "        loss_mat = criterion(preds, y).mul_(w)\n",
    "        loss = torch.sum(loss_mat)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]*len(x)\n",
    "        running_count += len(x)\n",
    "        t.set_postfix(loss=(running_loss/running_count))\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_count = 0\n",
    "    for batch in val_ext:\n",
    "        (x,xl) = get_ext_text(batch)\n",
    "        y = get_ext_labels(batch)\n",
    "        w = get_ext_mask(batch)\n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = F.sigmoid(model.decoder_three(model_out))\n",
    "        \n",
    "        loss_mat = criterion(preds, y).mul_(w)\n",
    "        loss = torch.sum(loss_mat)\n",
    "        \n",
    "        val_loss += loss.data[0]*len(x)\n",
    "        val_count += len(x)\n",
    "        \n",
    "    tox_val = validate_tox(model)\n",
    "    epoch_loss = running_loss / running_count\n",
    "\n",
    "    print('Epoch: {}, Train Loss: {:.5f}, Val Loss: {:.5f}, Tox AUC: {:.5f}'.format(epoch, epoch_loss, val_loss/val_count, tox_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2285d60cd9d847d7ba181cabd1bc8c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 0.04972, Val Loss: 0.04207, Val AUC: 0.98212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bb78f1df4548e396df4136d9efb7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, Train Loss: 0.03863, Val Loss: 0.03744, Val AUC: 0.98812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953947aad1cc4163ae957a2ea504e97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, Train Loss: 0.03425, Val Loss: 0.03842, Val AUC: 0.98851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dae26070534008a5cedf9df9f0f5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-07ed7c023af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_six\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5752385e8925>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, lengths)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcy_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreserve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             ))\n\u001b[1;32m    307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "epochs = 5\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "if use_cuda:\n",
    "    criterion=criterion.cuda()\n",
    "val_meter = MultiAUCMeter(6)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    model.train() \n",
    "    t = tqdm(train)\n",
    "    for batch in t:\n",
    "        (x,xl) = get_text(batch)\n",
    "        y = get_labels(batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = model.decoder_six(model_out)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]*len(x)\n",
    "        running_count += len(x)\n",
    "        t.set_postfix(loss=(running_loss/running_count))\n",
    "    \n",
    "    model.eval()\n",
    "    val_meter.reset()\n",
    "    val_loss = 0.0\n",
    "    val_count = 0\n",
    "    for batch in val:\n",
    "        (x,xl) = get_text(batch)\n",
    "        y = get_labels(batch)\n",
    "        model_out = model(x, lengths=xl)\n",
    "        preds = model.decoder_six(model_out)\n",
    "        \n",
    "        loss = criterion(preds, y)\n",
    "        \n",
    "        val_loss += loss.data[0]*len(x)\n",
    "        val_count += len(x)\n",
    "        \n",
    "        val_meter.add(preds,y)\n",
    "        \n",
    "    epoch_loss = running_loss / running_count\n",
    "\n",
    "    print('Epoch: {}, Train Loss: {:.5f}, Val Loss: {:.5f}, Val AUC: {:.5f}'.format(epoch, epoch_loss, val_loss/val_count, val_meter.avg()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9842980150327072,\n",
       " 0.99155026575566363,\n",
       " 0.99304883423953738,\n",
       " 0.99339808175244326,\n",
       " 0.98700536570157626,\n",
       " 0.99110479328874956]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_meter.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we define a quick convenience function to access the ids from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fields=[\n",
    "    (id_label, INDEX),\n",
    "    ('nid', None),\n",
    "    (text_label, TEXT)\n",
    "]\n",
    "test_data = ToxicDataset(\n",
    "            path=f'{path}{test_file}',\n",
    "            fields=test_fields\n",
    "        )\n",
    "INDEX.build_vocab(test_data)\n",
    "test = data.BucketIterator(test_data, batch_size=128,\n",
    "                                sort_key=lambda x: len(x.comment_text),\n",
    "                                sort_within_batch=True, train=False, repeat=False)\n",
    "\n",
    "def get_ids(batch):\n",
    "    return getattr(batch, id_label).data.cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b4eb27ddecd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}val.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mavg_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "val_raw = pd.read_csv(f'{path}val.csv')\n",
    "def score_val(val_raw, val_preds):\n",
    "    avg_auc = 0 \n",
    "    for i,label in enumerate(label_cols):\n",
    "        auc_score = roc_auc_score(val_raw[label].values,val_preds[:,i])\n",
    "        avg_auc += auc_score\n",
    "        print('{}: {}'.format(label, auc_score))\n",
    "    print('avg: {}'.format(avg_auc/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a47689be8b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_preds' is not defined"
     ]
    }
   ],
   "source": [
    "score_val(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def update_val(model, val_preds):\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for batch in val:\n",
    "        (x,xl) = get_text(batch)\n",
    "        ids = get_ids(batch)\n",
    "        preds=model.decoder_six(model(x,lengths=xl))\n",
    "        preds = preds.data.cpu().numpy()\n",
    "        preds = 1/(1+np.exp(-np.clip(preds,-10,10)))\n",
    "        val_preds[ids]+=preds\n",
    "    return val_preds\n",
    "def update_test(model, test_preds):\n",
    "    model.eval()\n",
    "    for batch in test:\n",
    "        (x,xl) = get_text(batch)\n",
    "        ids = get_ids(batch)\n",
    "        preds=model.decoder_six(model(x,lengths=xl))\n",
    "        preds = preds.data.cpu().numpy()\n",
    "        preds = 1/(1+np.exp(-np.clip(preds,-10,10)))\n",
    "        test_preds[ids]+=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_preds = []\n",
    "val_targets = []\n",
    "model.eval() # turn on evaluation mode\n",
    "for batch in val:\n",
    "    (x,xl) = get_text(batch)\n",
    "    y = get_labels(batch)\n",
    "    preds=model(x,lengths=xl)\n",
    "    preds = preds.data.cpu().numpy()\n",
    "    targets = y.data.cpu().numpy()\n",
    "    val_preds.append(preds[:,0])\n",
    "    val_targets.append(targets[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98500197729892547"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "preds_flat=np.hstack(val_preds)\n",
    "targets_flat=np.hstack(val_targets)\n",
    "roc_auc_score(targets_flat,preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def validate_tox(three_model):\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for batch in val:\n",
    "        (x,xl) = get_text(batch)\n",
    "        y = get_labels(batch)\n",
    "        preds=model(x,lengths=xl)\n",
    "        preds = preds.data.cpu().numpy()\n",
    "        targets = y.data.cpu().numpy()\n",
    "        val_preds.append(preds[:,0])\n",
    "        val_targets.append(targets[:,0])\n",
    "    preds_flat=np.hstack(val_preds)\n",
    "    targets_flat=np.hstack(val_targets)\n",
    "    return roc_auc_score(targets_flat,preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "def make_model(dl):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    nhidden=160\n",
    "    emsize=300\n",
    "    nlayers = 2\n",
    "    dropemb = 0.0\n",
    "    droprnn = 0.2\n",
    "    droplin = 0.0\n",
    "    model = RNNModel('GRU', dl.ntokens, emsize, nhidden, 16, 3, nlayers, dropemb=dropemb, droprnn=droprnn, droplin=droplin, bidirectional=True)\n",
    "    model.encoder.weight.data.copy_(dl.TEXT.vocab.vectors)\n",
    "    model.encoder.weight.requires_grad=False\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "    return model\n",
    "\n",
    "def run_iters(dl, model, epochs=5):\n",
    "    opts=get_opt(model)\n",
    "    criterion=nn.BCEWithLogitsLoss()\n",
    "    val_meter = MultiAUCMeter(6)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        running_count = 0\n",
    "        model.train() \n",
    "        t = tqdm(dl.train_both)\n",
    "        for batch in t:\n",
    "            loss_data = train_batch(model, batch, opts)\n",
    "            if (loss_data > 0):\n",
    "                running_loss += loss_data\n",
    "                running_count += 1\n",
    "                t.set_postfix(loss=(running_loss/running_count))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_count = 0\n",
    "        val_meter.reset()\n",
    "        for batch in val:\n",
    "            (x,xl) = get_text(batch)\n",
    "            y = get_labels(batch)\n",
    "            model_out = model(x, lengths=xl)\n",
    "            preds = model.decoder_six(model_out)\n",
    "\n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            val_loss += loss.data[0]*len(x)\n",
    "            val_count += len(x)\n",
    "\n",
    "            val_meter.add(preds,y)\n",
    "\n",
    "        epoch_loss = running_loss / running_count\n",
    "\n",
    "        print('Epoch: {}, Train Loss: {:.5f}, Val Loss: {:.5f}, Val AUC: {:.5f}'.format(epoch, epoch_loss, val_loss/val_count, val_meter.avg()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a0995ae31440acabc49d7d49dca980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 0.05951, Val Loss: 0.04520, Val AUC: 0.97821\n",
      "toxic: 0.9827326813307198\n",
      "severe_toxic: 0.9909718317807407\n",
      "obscene: 0.9900863439246017\n",
      "threat: 0.9544718767964661\n",
      "insult: 0.9837074805070043\n",
      "identity_hate: 0.967298269419227\n",
      "avg: 0.9782114139597932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2595173f343c48e18593819d300d5ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 0.06231, Val Loss: 0.04583, Val AUC: 0.98012\n",
      "toxic: 0.9831562624486331\n",
      "severe_toxic: 0.9905621730306595\n",
      "obscene: 0.9900159499836896\n",
      "threat: 0.961447459986082\n",
      "insult: 0.984004331942557\n",
      "identity_hate: 0.9701644428649889\n",
      "avg: 0.9798917700427682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfc054e1df942ceb0aea254603bdd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 0.06117, Val Loss: 0.04376, Val AUC: 0.97925\n",
      "toxic: 0.9834003211747316\n",
      "severe_toxic: 0.9910104985505187\n",
      "obscene: 0.9904072685801552\n",
      "threat: 0.9606078484765967\n",
      "insult: 0.9840482878335165\n",
      "identity_hate: 0.9713953413239428\n",
      "avg: 0.9801449276565769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9945f47df6c417ba4e0aef16149d9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 0.06104, Val Loss: 0.04529, Val AUC: 0.98008\n",
      "toxic: 0.9835877058610786\n",
      "severe_toxic: 0.9910136336940141\n",
      "obscene: 0.9907012057462972\n",
      "threat: 0.9605972587818826\n",
      "insult: 0.9843196806534908\n",
      "identity_hate: 0.9720500974595893\n",
      "avg: 0.9803782636993921\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da00234d14764f33a7e4671081ad64d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train Loss: 0.05887, Val Loss: 0.04592, Val AUC: 0.97796\n",
      "toxic: 0.9835267730784552\n",
      "severe_toxic: 0.9909300298674669\n",
      "obscene: 0.9907799978919563\n",
      "threat: 0.9603597470575777\n",
      "insult: 0.9843585647108781\n",
      "identity_hate: 0.9719429457564694\n",
      "avg: 0.9803163430604673\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros((len(test_data),6))\n",
    "val_preds = np.zeros((len(val_data),6))\n",
    "for i in range(5):\n",
    "    model = make_model()\n",
    "    run_iters(model, epochs=1)\n",
    "    update_val(model, val_preds)\n",
    "    score_val(val_preds)\n",
    "    update_test(model, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And go ahead and store the data in a matrix. Because we get the comments out of order, the ids help us reorder them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_preds = np.zeros((len(test_data), 6))\n",
    "model.eval()\n",
    "for batch in test:\n",
    "    (x,xl) = get_text(batch)\n",
    "    ids = get_ids(batch)\n",
    "    model_out=model(x,lengths=xl)\n",
    "    preds=model.decoder_six(model_out)\n",
    "    preds = preds.data.cpu().numpy()\n",
    "    preds = 1/(1+np.exp(-np.clip(preds,-10,10)))\n",
    "    test_preds[ids]=preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now reread the test file with pandas and write the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.440328</td>\n",
       "      <td>0.987529</td>\n",
       "      <td>0.116246</td>\n",
       "      <td>0.915932</td>\n",
       "      <td>0.163502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0.361152</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.001681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "      <td>0.035903</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...   \n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all.   \n",
       "5  0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "6  00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...   \n",
       "7  000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "8  00025358d4737918  \" \\n Only a fool can believe in such numbers. ...   \n",
       "9  00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...   \n",
       "\n",
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0  0.999079      0.440328  0.987529  0.116246  0.915932       0.163502  \n",
       "1  0.000187      0.000045  0.000045  0.000045  0.000045       0.000045  \n",
       "2  0.000261      0.000045  0.000062  0.000045  0.000138       0.000116  \n",
       "3  0.000072      0.000045  0.000045  0.000045  0.000045       0.000045  \n",
       "4  0.000784      0.000045  0.000055  0.000080  0.000135       0.000057  \n",
       "5  0.000188      0.000045  0.000045  0.000046  0.000086       0.000045  \n",
       "6  0.002832      0.000045  0.000100  0.000045  0.000124       0.000045  \n",
       "7  0.361152      0.000344  0.002454  0.000458  0.008306       0.001681  \n",
       "8  0.035903      0.000046  0.000354  0.000118  0.001758       0.000081  \n",
       "9  0.000232      0.000045  0.000045  0.000045  0.000045       0.000045  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f'{path}test.csv')\n",
    "for i, col in enumerate(label_cols):   \n",
    "    df[col] = test_preds[:, i]\n",
    "df.drop(text_label,axis=1).to_csv(\"submission_ext_gru_short.csv\",index=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180a2e56f6a7c517</td>\n",
       "      <td>GA Review\\n:This review is transcluded from Ta...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180abdd1373fd099</td>\n",
       "      <td>\"\\n Marx? that's just historic ignorance. Stal...</td>\n",
       "      <td>0.025586</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.001035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180b31f596d885b8</td>\n",
       "      <td>@ Good! Just tell me how delete my account so ...</td>\n",
       "      <td>0.147984</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.035831</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.000593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180c9b29c0d8c8e2</td>\n",
       "      <td>Sabata's Counterattack vs. Sabata's Revenge.\\n...</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180fcc239f7ebfc3</td>\n",
       "      <td>Pmanderson|PMAnderson]] 23:42, 16 December</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1819829ecc4e6c5e</td>\n",
       "      <td>Re: Question at my RfA\\nWhile I am admittedly ...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>181cf62d39ad3686</td>\n",
       "      <td>\"\\n\\nPlease stop. If you continue to vandalize...</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>181cfafb023c1891</td>\n",
       "      <td>Because you're a DUMBASS, MastCell, that's why.</td>\n",
       "      <td>0.951092</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.568394</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.737460</td>\n",
       "      <td>0.021998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1820bae92b22e1f8</td>\n",
       "      <td>Dear Pedant,\\n\\nI'll sign what I want to sign ...</td>\n",
       "      <td>0.011701</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>182510a56c64a5cd</td>\n",
       "      <td>Alphabetize track list \\n\\nThe track list shou...</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  180a2e56f6a7c517  GA Review\\n:This review is transcluded from Ta...   \n",
       "1  180abdd1373fd099  \"\\n Marx? that's just historic ignorance. Stal...   \n",
       "2  180b31f596d885b8  @ Good! Just tell me how delete my account so ...   \n",
       "3  180c9b29c0d8c8e2  Sabata's Counterattack vs. Sabata's Revenge.\\n...   \n",
       "4  180fcc239f7ebfc3         Pmanderson|PMAnderson]] 23:42, 16 December   \n",
       "5  1819829ecc4e6c5e  Re: Question at my RfA\\nWhile I am admittedly ...   \n",
       "6  181cf62d39ad3686  \"\\n\\nPlease stop. If you continue to vandalize...   \n",
       "7  181cfafb023c1891    Because you're a DUMBASS, MastCell, that's why.   \n",
       "8  1820bae92b22e1f8  Dear Pedant,\\n\\nI'll sign what I want to sign ...   \n",
       "9  182510a56c64a5cd  Alphabetize track list \\n\\nThe track list shou...   \n",
       "\n",
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0  0.000045      0.000045  0.000045  0.000045  0.000045       0.000045  \n",
       "1  0.025586      0.000045  0.002081  0.000086  0.003901       0.001035  \n",
       "2  0.147984      0.000462  0.003227  0.035831  0.005710       0.000593  \n",
       "3  0.000058      0.000045  0.000045  0.000045  0.000045       0.000045  \n",
       "4  0.008826      0.000083  0.002341  0.000149  0.001971       0.000663  \n",
       "5  0.000046      0.000045  0.000045  0.000045  0.000045       0.000045  \n",
       "6  0.002493      0.000045  0.000552  0.000084  0.000594       0.000045  \n",
       "7  0.951092      0.005244  0.568394  0.000678  0.737460       0.021998  \n",
       "8  0.011701      0.000045  0.000808  0.000045  0.001083       0.000045  \n",
       "9  0.000059      0.000045  0.000045  0.000045  0.000045       0.000045  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f'{path}val.csv')\n",
    "for i, col in enumerate(label_cols):   \n",
    "    df[col] = val_preds[:, i]\n",
    "df.drop(text_label,axis=1).to_csv(\"val_gru_2layer.csv\",index=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('vocab.txt','w')\n",
    "for word in TEXT.vocab.itos:\n",
    "    f.write(f'{word}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
