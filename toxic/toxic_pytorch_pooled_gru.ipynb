{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "id_label = 'id'\n",
    "text_label = 'comment_text'\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "path='./toxic-data/'\n",
    "train_file = 'official_train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "# some iterators produce StopIteration, which is no longer a warning, we don't need to hear about it\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default dataset reader doesn't like the encoding of the toxic data, so we need our own dataset definition, just to set the CSV encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io,os,csv\n",
    "\n",
    "class ToxicDataset(data.Dataset):\n",
    "    \"\"\"Defines a Dataset of columns stored in CSV format.\"\"\"\n",
    "\n",
    "    def __init__(self, path, fields, skip_header=True, **kwargs):\n",
    "        with io.open(os.path.expanduser(path), encoding=\"utf8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "                \n",
    "            if skip_header:\n",
    "                next(reader)\n",
    "\n",
    "            examples = [data.Example.fromlist(line, fields) for line in reader]\n",
    "\n",
    "        super(ToxicDataset, self).__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fields:\n",
    "- lower: converts text to lowercase\n",
    "- include_lengths: includes a separate array with the length\n",
    "- tokenize: set to 'spacy' to use spacy tokenizer, requires spacy to be installed. Otherwise will just split on whitespace.\n",
    "- fix_length: Will buffer or trim fields to this length. Not required but speeds up processing significantly from trimming super long comments\n",
    "- sequential: if False, won't do any tokenization (whole field is the token)\n",
    "- use_vocab: if False, data must be numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the types of fields\n",
    "# pip install spacy for the tokenizer to work (or remove to use default)\n",
    "TEXT = data.Field(lower=True, include_lengths=True, fix_length=150, tokenize='spacy')\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# we use the index field to re-sort test data after processing\n",
    "INDEX = data.Field(sequential=False)\n",
    "\n",
    "train_fields=[\n",
    "    (id_label, INDEX),\n",
    "    (text_label, TEXT)\n",
    "]\n",
    "for label in label_cols:\n",
    "    train_fields.append((label,LABEL))\n",
    "\n",
    "train_data = ToxicDataset(\n",
    "            path=f'{path}{train_file}',\n",
    "            fields=train_fields\n",
    "        )\n",
    "\n",
    "test_fields=[\n",
    "    (id_label, INDEX),\n",
    "    (text_label, TEXT)\n",
    "]\n",
    "test_data = ToxicDataset(\n",
    "            path=f'{path}{test_file}',\n",
    "            fields=test_fields\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocab, for the index it will numericalize. For the text, we can choose from predefined vectors:\n",
    "- charngram.100d\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d\n",
    "- glove.twitter.27B.50d\n",
    "- glove.twitter.27B.100d\n",
    "- glove.twitter.27B.200d\n",
    "- glove.6B.50d\n",
    "- glove.6B.100d\n",
    "- glove.6B.200d\n",
    "- glove.6B.300d\n",
    "\n",
    "The proper files will be downloaded to .vector_cache/ if necessary.\n",
    "\n",
    "Worth noticing that a few extra tokens are tacked on: `<unk>` and `<pad>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntokens 50002\n"
     ]
    }
   ],
   "source": [
    "# This will download the glove vectors, see torchtext source for other options\n",
    "max_size = 50000\n",
    "TEXT.build_vocab(train_data, test_data, vectors='glove.42B.300d', max_size=max_size)\n",
    "INDEX.build_vocab(test_data)\n",
    "\n",
    "# print vocab information\n",
    "ntokens = len(TEXT.vocab)\n",
    "print('ntokens', ntokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BucketIterator will shuffle the data and produce batches with sequences of roughly the same length. If we didn't want to split into epochs, we could set repeat=True and run for a set number of batches (rather than epochs). Must have `sort_within_batch=True` to use the lengths we picked up earlier.\n",
    "\n",
    "We also define convenience methods to access the comment text and labels from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data.BucketIterator(train_data, batch_size=32,\n",
    "                                sort_key=lambda x: len(x.comment_text),\n",
    "                                sort_within_batch=True, repeat=False)\n",
    "test = data.BucketIterator(test_data, batch_size=128,\n",
    "                                sort_key=lambda x: len(x.comment_text),\n",
    "                                sort_within_batch=True, train=False, repeat=False)\n",
    "\n",
    "def get_text(batch):\n",
    "    return getattr(batch, text_label)\n",
    "def get_labels(batch):\n",
    "    # Get the labels as one tensor from the batch object\n",
    "    return torch.cat([getattr(batch, label).unsqueeze(1) for label in label_cols], dim=1).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the meat of the model. A few points to notice in `__init__`:\n",
    "- `Dropout2d` is a spatial dropout function, which will drop entire layers (rather than just individial connections). It doesn't necessarily require 2d data\n",
    "- We define `self.rnns` as a ModuleList so that all of the sub-components will be discovered properly\n",
    "- The pools require an argument that is number of output segments, but we just want a global one for each avg/max\n",
    "\n",
    "and in `forward`:\n",
    "- We move to/from a packed sequence for the rnn section if we have the lengths\n",
    "- We need to rearrange the output of the rnn to have sequence last for pooling layers\n",
    "- We don't have a sigmoid output because we will later use a special loss function that takes the logit output directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nout, nlayers, dropemb=0.2, droprnn=0.0, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.drop = nn.Dropout2d(dropemb)\n",
    "        self.ndir = 2 if bidirectional else 1\n",
    "        assert rnn_type in ['LSTM', 'GRU'], 'RNN type is not supported'\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid*self.ndir, nhid, 1, dropout=droprnn, bidirectional=bidirectional) for l in range(nlayers)]\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnns = [torch.nn.GRU(ninp if l == 0 else nhid*self.ndir, nhid, 1, dropout=droprnn, bidirectional=bidirectional) for l in range(nlayers)]\n",
    "        \n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.avg_pool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = torch.nn.AdaptiveMaxPool1d(1)\n",
    "        self.decoder = nn.Linear(nhid*self.ndir*2, nout)\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "        emb = self.encoder(input)\n",
    "        \n",
    "        raw_output = self.drop(emb)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            lengths = lengths.view(-1).tolist()\n",
    "            raw_output = nn.utils.rnn.pack_padded_sequence(raw_output, lengths)\n",
    "            \n",
    "        for rnn in self.rnns:\n",
    "            raw_output,_ = rnn(raw_output)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            raw_output, lengths = nn.utils.rnn.pad_packed_sequence(raw_output)\n",
    "            \n",
    "        bsz = raw_output.size(1)\n",
    "        rnn_avg = self.avg_pool(raw_output.permute(1,2,0))\n",
    "        rnn_max = self.max_pool(raw_output.permute(1,2,0))\n",
    "        rnn_out = torch.cat([rnn_avg.view(bsz,-1),rnn_max.view(bsz,-1)], dim=1)\n",
    "            \n",
    "        result = self.decoder(rnn_out)\n",
    "        return self.decoder(rnn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are parameters from other example kernels--not necessarily optimized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "nhidden=80\n",
    "emsize=300\n",
    "nlayers = 1\n",
    "dropemb = 0.2\n",
    "droprnn = 0.1\n",
    "model = RNNModel('GRU', ntokens, emsize, nhidden, 6, nlayers, dropemb=dropemb, droprnn=droprnn, bidirectional=True)\n",
    "model.encoder.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))\n",
    "if use_cuda:\n",
    "    model=model.cuda()\n",
    "    criterion=criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main pytorch training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dd33ff150c48cdb639810a3829e23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Loss: 0.05571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8012ee2b5e440288bda2da13cf9cc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, Loss: 0.04746\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    model.train() \n",
    "    t = tqdm(train)\n",
    "    for batch in t:\n",
    "        (x,xl) = get_text(batch)\n",
    "        y = get_labels(batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(x, lengths=xl)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]*len(x)\n",
    "        running_count += len(x)\n",
    "        t.set_postfix(loss=(running_loss/running_count))\n",
    "\n",
    "    epoch_loss = running_loss / running_count\n",
    "\n",
    "    print('Epoch: {}, Loss: {:.5f}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we define a quick convenience function to access the ids from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(batch):\n",
    "    return getattr(batch, id_label).data.cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And go ahead and store the data in a matrix. Because we get the comments out of order, the ids help us reorder them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_preds = np.zeros((len(INDEX.vocab), 6))\n",
    "model.eval()\n",
    "for batch in test:\n",
    "    (x,xl) = get_text(batch)\n",
    "    ids = get_ids(batch)\n",
    "    preds=model(x,lengths=xl)\n",
    "    preds = preds.data.cpu().numpy()\n",
    "    preds = 1/(1+np.exp(-np.clip(preds,-10,10)))\n",
    "    test_preds[ids]=preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now reread the test file with pandas and write the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.996662</td>\n",
       "      <td>0.342365</td>\n",
       "      <td>0.965199</td>\n",
       "      <td>0.040285</td>\n",
       "      <td>0.950952</td>\n",
       "      <td>0.419067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>0.013698</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.000304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0.130177</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.019207</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.015886</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...   \n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all.   \n",
       "5  0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "6  00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...   \n",
       "7  000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "8  00025358d4737918  \" \\n Only a fool can believe in such numbers. ...   \n",
       "9  00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...   \n",
       "\n",
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0  0.996662      0.342365  0.965199  0.040285  0.950952       0.419067  \n",
       "1  0.000932      0.000087  0.002012  0.000045  0.000621       0.000047  \n",
       "2  0.004811      0.000449  0.005966  0.000064  0.001308       0.000119  \n",
       "3  0.000379      0.000045  0.001395  0.000048  0.000376       0.000074  \n",
       "4  0.013698      0.000383  0.008281  0.000547  0.002025       0.000304  \n",
       "5  0.000453      0.000045  0.001098  0.000128  0.000508       0.000140  \n",
       "6  0.001435      0.000045  0.001234  0.000045  0.000706       0.000155  \n",
       "7  0.130177      0.001079  0.019207  0.000294  0.015886       0.000912  \n",
       "8  0.013553      0.000051  0.005606  0.000045  0.003323       0.000405  \n",
       "9  0.000202      0.000045  0.000596  0.000077  0.000138       0.000045  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f'{path}{test_file}')\n",
    "for i, col in enumerate(label_cols):   \n",
    "    df[col] = test_preds[1:, i]\n",
    "df.drop(text_label,axis=1).to_csv(\"submission.csv\",index=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
